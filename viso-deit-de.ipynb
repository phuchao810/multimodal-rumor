{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12127202,"sourceType":"datasetVersion","datasetId":7536865},{"sourceId":12277883,"sourceType":"datasetVersion","datasetId":7737299},{"sourceId":12297259,"sourceType":"datasetVersion","datasetId":7750713},{"sourceId":12308836,"sourceType":"datasetVersion","datasetId":7758409},{"sourceId":12409262,"sourceType":"datasetVersion","datasetId":7825990}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:04:40.552597Z","iopub.execute_input":"2025-09-11T02:04:40.553181Z","iopub.status.idle":"2025-09-11T02:04:40.556598Z","shell.execute_reply.started":"2025-09-11T02:04:40.553154Z","shell.execute_reply":"2025-09-11T02:04:40.555766Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:04:40.558010Z","iopub.execute_input":"2025-09-11T02:04:40.558288Z","iopub.status.idle":"2025-09-11T02:04:40.568960Z","shell.execute_reply.started":"2025-09-11T02:04:40.558260Z","shell.execute_reply":"2025-09-11T02:04:40.568132Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_posts_from_csv(csv_path):\n    df = pd.read_csv(csv_path)\n    posts = []\n    for _, row in df.iterrows():\n        post_id = str(row[\"org_index\"])\n        image_path = f\"/kaggle/input/nckh123/IMG_DATA/IMG_DATA/{post_id}.png\"\n\n        post = {\n            \"id\": post_id,\n            \"title\": str(row[\"title\"]),\n            \"content\": str(row[\"content\"]),\n            \"label\": str(row[\"Final_label\"]).lower(),\n            \"image\": image_path,\n            \"explanation\": str(row[\"Claude_reason\"])  # hoặc \"claude_explanation\"\n        }\n        posts.append(post)\n    return posts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:04:40.569808Z","iopub.execute_input":"2025-09-11T02:04:40.570290Z","iopub.status.idle":"2025-09-11T02:04:40.579491Z","shell.execute_reply.started":"2025-09-11T02:04:40.570265Z","shell.execute_reply":"2025-09-11T02:04:40.578832Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_metadata_only(metadata_root):\n    metadata_all = {}\n\n    for pid in sorted(os.listdir(metadata_root)):\n        path = os.path.join(metadata_root, pid)\n        if not os.path.isdir(path):\n            continue\n\n        related_articles = []\n\n        for sub_idx in sorted(os.listdir(path)):\n            sub_path = os.path.join(path, sub_idx)\n            if not os.path.isdir(sub_path):\n                continue\n\n            try:\n                with open(os.path.join(sub_path, \"title.txt\"), encoding=\"utf-8\") as f:\n                    title = f.read().strip()\n                with open(os.path.join(sub_path, \"content.txt\"), encoding=\"utf-8\") as f:\n                    content = f.read().strip()\n            except:\n                continue\n\n            image_paths = [\n                os.path.join(sub_path, img)\n                for img in sorted(os.listdir(sub_path))\n                if img.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n            ]\n\n            related_articles.append({\n                \"title\": title,\n                \"content\": content,\n                \"images\": image_paths\n            })\n\n        # ✅ Chỉ thêm vào dict nếu có ít nhất 1 bài báo phụ\n        if related_articles:\n            metadata_all[pid] = related_articles\n\n    return metadata_all","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:04:40.580889Z","iopub.execute_input":"2025-09-11T02:04:40.581109Z","iopub.status.idle":"2025-09-11T02:04:40.592103Z","shell.execute_reply.started":"2025-09-11T02:04:40.581091Z","shell.execute_reply":"2025-09-11T02:04:40.591352Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#Load data \ntrain_set = load_posts_from_csv('/kaggle/input/nckh123/train.csv')\ndev_set = load_posts_from_csv('/kaggle/input/nckh123/val.csv')\ntest_set = load_posts_from_csv('/kaggle/input/nckh123/test.csv')\nmetadata = load_metadata_only('/kaggle/input/envide/crawl_articles')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:04:40.592856Z","iopub.execute_input":"2025-09-11T02:04:40.593091Z","iopub.status.idle":"2025-09-11T02:06:31.651341Z","shell.execute_reply.started":"2025-09-11T02:04:40.593064Z","shell.execute_reply":"2025-09-11T02:06:31.650534Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# Load model text embedding (BERT, PhoBERT,...)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"uitnlp/visobert\"  # thay bằng model tiếng Việt nếu cần\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntext_model = AutoModel.from_pretrained(model_name).to(device)\ntext_model.eval()\n\ndef get_text_embedding(text):\n    \"\"\"Trả về embedding mean pooling cho một đoạn văn bản\"\"\"\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n    with torch.no_grad():\n        outputs = text_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze()  # (hidden_size,)\n    \ndef text_emb(maindata, metadata, tokenizer, model):\n    org_embeddings = {}\n\n    for idx, row in maindata.iterrows():\n        pid = str(row[\"id\"])\n\n        # Tập hợp text chính và các evidence\n        texts = [f\"{row['title']} {row['content']}\".strip()]\n        if pid in metadata:\n            for item in metadata[pid]:\n                texts.append(f\"{item['title']} {item['content']}\".strip())\n            org_embeddings[pid] = texts\n        #Tokenize & lấy embedding\n        inputs = tokenizer(texts, padding=True, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            emb = outputs.last_hidden_state.mean(dim=1)  # embedding từng văn bản\n\n        # Gộp mean theo org_index\n        org_embeddings[pid] = emb.mean(dim=0)  # (hidden_size,)\n\n    return org_embeddings  # dict: {org_index: embedding}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:11:03.575840Z","iopub.execute_input":"2025-09-11T02:11:03.576461Z","iopub.status.idle":"2025-09-11T02:11:12.407329Z","shell.execute_reply.started":"2025-09-11T02:11:03.576435Z","shell.execute_reply":"2025-09-11T02:11:12.404742Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7d2b0a2ec5942b3991329b4abe2e67a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/471k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"396b48740f384982a1ab3dfd19410ff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fe30c4b9ad64f0094eef1a0edb6542d"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9fa51419714a73a7ca7d64b64d47ba"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 1. Load ResNet-50 pretrained\nmodel_name = \"facebook/deit-base-distilled-patch16-224\"\nfeature_extractor = DeiTFeatureExtractor.from_pretrained(model_name)\nimg_model = DeiTModel.from_pretrained(model_name).to(device).eval()\n\n# 2. Transform cho ảnh\nimg_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # resize chuẩn\n    transforms.ToTensor(),          # thành tensor\n    transforms.Normalize(           # normalize theo ImageNet\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\ndef get_image_embedding(img_path):\n    \"\"\"Trả về embedding (768,) của ảnh từ DeiT\"\"\"\n    image = Image.open(img_path).convert(\"RGB\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = img_model(**inputs)\n        # outputs.last_hidden_state shape: [1, num_patches+1, hidden_size]\n        emb = outputs.last_hidden_state[:, 0, :]  # lấy CLS token (1, 768)\n    return emb.squeeze(0).cpu()  # (768,)\n\ndef img_emb(maindata, metadata):\n    org_img_embeddings = {}\n\n    for idx, row in maindata.iterrows():\n        pid = str(row[\"id\"])\n\n        # --- Ảnh chính ---\n        try:\n            main_emb = get_image_embedding(row['image'])\n        except Exception as e:\n            print(f\"⚠️ Lỗi ảnh chính {row['image']}: {e}\")\n            main_emb = None\n\n        # --- Ảnh evidence ---\n        evidence_embs = []\n        if pid in metadata:\n            for item in metadata[pid]:\n                img_paths = item['images']  # list path ảnh evidence\n                emb_list = []\n                for path in img_paths:\n                    try:\n                        emb_list.append(get_image_embedding(path))\n                    except Exception as e:\n                        print(f\"⚠️ Lỗi ảnh {path}: {e}\")\n                if emb_list:\n                    # lấy trung bình trong từng evidence group\n                    evidence_embs.append(torch.stack(emb_list).mean(dim=0))\n\n        # --- Trung bình main + evidence ---\n        all_embs = []\n        if main_emb is not None:\n            all_embs.append(main_emb)\n        if evidence_embs:\n            all_embs.extend(evidence_embs)\n\n        if all_embs:\n            org_img_embeddings[pid] = torch.stack(all_embs).mean(dim=0)\n        else:\n            org_img_embeddings[pid] = None\n\n    return org_img_embeddings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:10:54.691294Z","iopub.execute_input":"2025-09-11T02:10:54.691586Z","iopub.status.idle":"2025-09-11T02:10:55.249004Z","shell.execute_reply.started":"2025-09-11T02:10:54.691566Z","shell.execute_reply":"2025-09-11T02:10:55.248201Z"}},"outputs":[{"name":"stderr","text":"Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n\n\n# text_emb = text_emb(main, metadata, tokenizer, text_model)\n# img_emb = img_emb(main, metadata)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:06:50.118245Z","iopub.status.idle":"2025-09-11T02:06:50.118603Z","shell.execute_reply.started":"2025-09-11T02:06:50.118414Z","shell.execute_reply":"2025-09-11T02:06:50.118433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set = pd.DataFrame(train_set)\ndev_set = pd.DataFrame(dev_set)\ntest_set = pd.DataFrame(test_set)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:11:26.291031Z","iopub.execute_input":"2025-09-11T02:11:26.291344Z","iopub.status.idle":"2025-09-11T02:11:26.306249Z","shell.execute_reply.started":"2025-09-11T02:11:26.291320Z","shell.execute_reply":"2025-09-11T02:11:26.305376Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_text = text_emb(train_set, metadata, tokenizer, text_model)\ntrain_img = img_emb(train_set, metadata)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:12:06.066545Z","iopub.execute_input":"2025-09-11T02:12:06.067162Z","iopub.status.idle":"2025-09-11T02:32:35.433045Z","shell.execute_reply.started":"2025-09-11T02:12:06.067137Z","shell.execute_reply":"2025-09-11T02:32:35.432449Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (91202500 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (122987520 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"dev_text = text_emb(dev_set, metadata, tokenizer, text_model)\ndev_img = img_emb(dev_set, metadata)\n\ntest_text = text_emb(test_set, metadata, tokenizer, text_model)\ntest_img = img_emb(test_set, metadata)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:32:35.434146Z","iopub.execute_input":"2025-09-11T02:32:35.434389Z","iopub.status.idle":"2025-09-11T02:47:04.692517Z","shell.execute_reply.started":"2025-09-11T02:32:35.434372Z","shell.execute_reply":"2025-09-11T02:47:04.691960Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass FusionEmbeddingDataset(Dataset):\n    def __init__(self, main_df, text_emb_dict, img_emb_dict, id_col=\"id\", label_col=\"label\"):\n        self.ids = main_df[id_col].tolist()\n        self.labels = main_df[label_col].tolist()\n        self.text_emb_dict = text_emb_dict\n        self.img_emb_dict = img_emb_dict\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        pid = str(self.ids[idx])\n        label = torch.tensor(float(self.labels[idx]), dtype=torch.float)\n    \n        text_val = self.text_emb_dict[pid]\n        img_val  = self.img_emb_dict[pid]\n    \n        # Đảm bảo trả ra tensor đúng dtype\n        text_emb = torch.as_tensor(text_val, dtype=torch.float)\n        img_emb  = torch.as_tensor(img_val, dtype=torch.float)\n    \n        return text_emb, img_emb, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:04.693179Z","iopub.execute_input":"2025-09-11T02:47:04.693360Z","iopub.status.idle":"2025-09-11T02:47:04.699325Z","shell.execute_reply.started":"2025-09-11T02:47:04.693346Z","shell.execute_reply":"2025-09-11T02:47:04.698810Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass EarlyFusionModel(nn.Module):\n    def __init__(self, text_dim, img_dim, hidden_size=768, dropout_rate=0.2):\n        \"\"\"\n        text_dim: chiều embedding text (VD: 768 cho ViSOBERT, ViT5)\n        img_dim: chiều embedding ảnh (VD: 2048 cho ResNet50)\n        hidden_size: chiều không gian chung sau khi chiếu\n        \"\"\"\n        super(EarlyFusionModel, self).__init__()\n\n        # Projection để đưa cả hai về cùng kích thước hidden_size\n        self.text_proj = nn.Linear(text_dim, hidden_size)\n        self.img_proj = nn.Linear(img_dim, hidden_size)\n\n        # Shared Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_size, nhead=8,\n            dim_feedforward=2048, dropout=dropout_rate,\n            batch_first=False\n        )\n        self.shared_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_size, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, text_emb, img_emb):\n        \"\"\"\n        text_emb: [B, text_dim]\n        img_emb: [B, img_dim]\n        \"\"\"\n        # 1. Projection\n        t_proj = self.text_proj(text_emb).unsqueeze(1)  # [B, 1, H]\n        i_proj = self.img_proj(img_emb).unsqueeze(1)    # [B, 1, H]\n\n        # 2. Ghép lại sequence: [B, 2, H] -> [2, B, H]\n        fusion = torch.cat([i_proj, t_proj], dim=1).transpose(0, 1)\n\n        # 3. Transformer encoder\n        encoded = self.shared_encoder(fusion)  # [2, B, H]\n\n        # 4. Lấy token ảnh (vị trí 0) làm đại diện\n        cls_token = encoded[0]  # [B, H]\n\n        # 5. Classifier\n        output = self.classifier(cls_token)  # [B, 1]\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:04.700080Z","iopub.execute_input":"2025-09-11T02:47:04.700349Z","iopub.status.idle":"2025-09-11T02:47:04.712513Z","shell.execute_reply.started":"2025-09-11T02:47:04.700332Z","shell.execute_reply":"2025-09-11T02:47:04.712005Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Subset\n\n# Tạo dataset\ntrain = FusionEmbeddingDataset(train_set, train_text, train_img)\nval = FusionEmbeddingDataset(dev_set, dev_text, dev_img)\ntest = FusionEmbeddingDataset(test_set, test_text, test_img)\n\n# DataLoader\ntrain_loader = DataLoader(train, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val, batch_size=32, shuffle=False)\ntest_loader  = DataLoader(test, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:04.714260Z","iopub.execute_input":"2025-09-11T02:47:04.714863Z","iopub.status.idle":"2025-09-11T02:47:04.738574Z","shell.execute_reply.started":"2025-09-11T02:47:04.714846Z","shell.execute_reply":"2025-09-11T02:47:04.737949Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = EarlyFusionModel(text_dim=768, img_dim=768, hidden_size=768).to(device)\ncriterion = nn.BCEWithLogitsLoss()   # nhị phân\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:04.739291Z","iopub.execute_input":"2025-09-11T02:47:04.739521Z","iopub.status.idle":"2025-09-11T02:47:04.819674Z","shell.execute_reply.started":"2025-09-11T02:47:04.739497Z","shell.execute_reply":"2025-09-11T02:47:04.818993Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"for epoch in range(20):  # số epoch ví dụ\n    # --- Training ---\n    model.train()\n    total_loss = 0\n    for text_emb, img_emb, label in train_loader:\n        text_emb, img_emb, label = text_emb.to(device), img_emb.to(device), label.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(text_emb, img_emb).squeeze(1)  # [B]\n        loss = criterion(outputs, label)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    avg_train_loss = total_loss / len(train_loader)\n\n    # --- Validation ---\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for text_emb, img_emb, label in val_loader:\n            text_emb, img_emb, label = text_emb.to(device), img_emb.to(device), label.to(device)\n            outputs = model(text_emb, img_emb).squeeze(1)\n\n            loss = criterion(outputs, label)\n            val_loss += loss.item()\n\n            # nếu là phân loại nhị phân (sigmoid đầu ra)\n            preds = (outputs > 0.5).long()\n            correct += (preds == label).sum().item()\n            total += label.size(0)\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_acc = correct / total\n\n    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} | \"\n          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:04.820323Z","iopub.execute_input":"2025-09-11T02:47:04.820521Z","iopub.status.idle":"2025-09-11T02:47:29.967772Z","shell.execute_reply.started":"2025-09-11T02:47:04.820504Z","shell.execute_reply":"2025-09-11T02:47:29.966949Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 - Train Loss: 0.4695 | Val Loss: 0.3609 | Val Acc: 0.8160\nEpoch 2 - Train Loss: 0.3932 | Val Loss: 0.3705 | Val Acc: 0.8400\nEpoch 3 - Train Loss: 0.3620 | Val Loss: 0.3783 | Val Acc: 0.8420\nEpoch 4 - Train Loss: 0.3144 | Val Loss: 0.3995 | Val Acc: 0.8500\nEpoch 5 - Train Loss: 0.2792 | Val Loss: 0.3798 | Val Acc: 0.8460\nEpoch 6 - Train Loss: 0.2478 | Val Loss: 0.5144 | Val Acc: 0.8280\nEpoch 7 - Train Loss: 0.2133 | Val Loss: 0.4937 | Val Acc: 0.8400\nEpoch 8 - Train Loss: 0.2290 | Val Loss: 0.4832 | Val Acc: 0.8520\nEpoch 9 - Train Loss: 0.1860 | Val Loss: 0.5232 | Val Acc: 0.8440\nEpoch 10 - Train Loss: 0.1807 | Val Loss: 0.5704 | Val Acc: 0.8420\nEpoch 11 - Train Loss: 0.1584 | Val Loss: 0.4542 | Val Acc: 0.8260\nEpoch 12 - Train Loss: 0.1563 | Val Loss: 0.4593 | Val Acc: 0.8400\nEpoch 13 - Train Loss: 0.1545 | Val Loss: 0.6011 | Val Acc: 0.8480\nEpoch 14 - Train Loss: 0.1375 | Val Loss: 0.5912 | Val Acc: 0.8440\nEpoch 15 - Train Loss: 0.1353 | Val Loss: 0.5605 | Val Acc: 0.8400\nEpoch 16 - Train Loss: 0.1197 | Val Loss: 0.5941 | Val Acc: 0.8380\nEpoch 17 - Train Loss: 0.0998 | Val Loss: 0.6209 | Val Acc: 0.8380\nEpoch 18 - Train Loss: 0.0895 | Val Loss: 0.6532 | Val Acc: 0.8560\nEpoch 19 - Train Loss: 0.0959 | Val Loss: 0.5142 | Val Acc: 0.8380\nEpoch 20 - Train Loss: 0.1134 | Val Loss: 0.5666 | Val Acc: 0.8540\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# for epoch in range(5):  # số epoch ví dụ\n#     model.train()\n#     total_loss = 0\n#     for text_emb, img_emb, label in train_loader:\n#         text_emb, img_emb, label = text_emb.to(device), img_emb.to(device), label.to(device)\n\n#         optimizer.zero_grad()\n#         outputs = model(text_emb, img_emb).squeeze(1)  # [B]\n#         loss = criterion(outputs, label)\n#         loss.backward()\n#         optimizer.step()\n\n#         total_loss += loss.item()\n#     print(f\"Epoch {epoch+1} - Train Loss: {total_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:29.968547Z","iopub.execute_input":"2025-09-11T02:47:29.968778Z","iopub.status.idle":"2025-09-11T02:47:29.972199Z","shell.execute_reply.started":"2025-09-11T02:47:29.968753Z","shell.execute_reply":"2025-09-11T02:47:29.971477Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"model.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for text_emb, img_emb, label in val_loader:\n        text_emb, img_emb, label = text_emb.to(device), img_emb.to(device), label.to(device)\n        outputs = model(text_emb, img_emb).squeeze(1)\n        preds = torch.sigmoid(outputs) > 0.5\n        correct += (preds == label.int()).sum().item()\n        total += label.size(0)\n\nprint(f\"Validation Accuracy: {correct/total:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:29.972919Z","iopub.execute_input":"2025-09-11T02:47:29.973079Z","iopub.status.idle":"2025-09-11T02:47:30.021501Z","shell.execute_reply.started":"2025-09-11T02:47:29.973066Z","shell.execute_reply":"2025-09-11T02:47:30.020828Z"}},"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.8480\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import random\n\n# Lấy random index trong test set\nidx = random.randint(0, len(test) - 1)\n\n# Lấy sample từ dataset\ntext_emb, img_emb, label = test[idx]\npid = test.ids[idx]   # nếu FusionEmbeddingDataset có self.ids\n\nprint(f\"Sample ID: {pid}\")\nprint(f\"Label: {label.item()}\")\nprint(f\"Text embedding shape: {text_emb.shape}\")\nprint(f\"Image embedding shape: {img_emb.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:30.022240Z","iopub.execute_input":"2025-09-11T02:47:30.022476Z","iopub.status.idle":"2025-09-11T02:47:30.027291Z","shell.execute_reply.started":"2025-09-11T02:47:30.022455Z","shell.execute_reply":"2025-09-11T02:47:30.026464Z"}},"outputs":[{"name":"stdout","text":"Sample ID: 4391\nLabel: 1.0\nText embedding shape: torch.Size([768])\nImage embedding shape: torch.Size([768])\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"text_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T02:47:30.027961Z","iopub.execute_input":"2025-09-11T02:47:30.028165Z","iopub.status.idle":"2025-09-11T02:47:30.183885Z","shell.execute_reply.started":"2025-09-11T02:47:30.028143Z","shell.execute_reply":"2025-09-11T02:47:30.183219Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([ 2.9355e-01,  1.3075e-01, -1.9709e-02,  2.6250e-02,  1.0093e-03,\n        -6.2689e-01,  2.4840e-01,  3.4099e-01, -1.4325e-01, -5.8854e-01,\n        -1.9034e-01, -1.0828e-01, -1.1120e-01, -1.3024e+00,  3.5936e-01,\n         3.7063e-01, -1.6561e-01, -4.7806e-01, -7.3034e-01, -6.7138e-01,\n         5.5841e-02,  4.1499e-01, -5.6283e-01, -1.4150e-01,  2.7962e-01,\n         2.3754e-01,  2.1334e-01,  9.0556e-02, -1.6093e-01,  7.3268e-01,\n        -6.0794e-01,  2.7506e-01,  2.9857e-01,  6.1257e-01,  4.2190e-01,\n         7.0229e-01, -5.9661e-01,  5.0705e-01, -7.5005e-02, -2.3987e-02,\n         5.3443e-01, -9.8235e-01, -8.6952e-01,  2.1556e-01, -8.5297e-01,\n         2.4150e-01, -2.3121e-01,  1.8493e-01,  6.7805e-01,  2.2387e-01,\n        -3.5422e-01,  4.2971e-01, -2.6120e-01, -2.6230e-03, -7.5583e-01,\n        -5.9578e-01, -8.9433e-02, -8.2594e-01,  2.4971e-01, -1.2534e+00,\n        -4.0194e-03,  6.4654e-01,  6.1894e-02, -1.7787e-01, -1.2918e+00,\n        -4.6381e-01, -4.8205e-01, -3.1871e-01,  6.6384e-01, -2.4047e-02,\n        -4.2826e-02, -3.2948e-01,  6.3680e-02, -2.6808e-02, -1.9616e-01,\n         2.6063e-01,  1.9483e-01, -7.7450e-03, -1.8319e-01,  1.1117e-01,\n        -5.0313e-01, -6.8249e-01, -5.8911e-01,  7.6625e-01, -6.7325e-01,\n        -2.6515e-02, -5.5559e-01, -2.2398e-01,  8.3959e-01,  8.5971e-01,\n        -5.1954e-01,  2.0008e-03,  1.3445e+00,  2.7284e-01, -3.6197e-01,\n         4.0767e-01,  6.0517e-01,  2.9216e-01,  1.6833e+00, -5.7447e-01,\n         6.0707e-01, -7.1879e-01,  2.4032e-01, -4.0974e-02,  2.2883e-01,\n        -9.3447e-01, -3.2088e-01, -5.8443e-01, -3.0288e-01, -2.0783e-01,\n         8.1197e-01,  4.7302e-01,  2.1473e-01,  1.2874e+00,  1.7299e-01,\n         2.4353e-01, -8.6234e-01,  3.6353e-02, -1.1551e-01, -1.3070e+00,\n        -4.4721e-01,  1.2823e+00, -3.9954e-01,  5.8858e-01,  1.1214e-01,\n        -1.3423e+00, -1.3967e-01, -3.9559e-01,  1.4925e+00, -8.6996e-01,\n        -3.1332e-01, -1.0122e+00,  1.1746e+00, -8.6403e-01, -1.1248e-01,\n        -6.4996e-01,  4.3061e-01,  4.5978e-01,  1.9783e-01, -1.6125e-01,\n         7.1430e-01, -1.5401e-01, -3.1013e-01, -6.9551e-01,  4.3830e-01,\n         1.2540e-01,  1.0823e+00, -1.6248e-01, -3.5899e-03,  1.6974e-01,\n         7.3481e-02, -1.6876e-01,  2.9458e-02, -5.6390e-02, -3.9475e-01,\n        -1.4980e-01, -8.9420e-02, -3.1003e-02,  4.6825e-02,  6.4199e-01,\n         1.9648e-01, -3.4992e-01,  5.7871e-01, -7.1231e-01, -1.9122e-01,\n        -1.1409e+00, -2.6285e-01,  1.2524e+00, -1.0772e-01, -9.0552e-01,\n        -4.9702e-01,  1.4969e+00, -7.4118e-01, -6.7951e-02, -1.8091e-01,\n        -4.3917e-01,  6.3860e-01,  4.9754e-01, -1.0613e-01,  1.0684e+00,\n         1.0709e+00,  2.3331e-02,  8.4847e-01, -9.4741e-01,  1.4463e+00,\n        -2.0354e-01, -1.7486e-01, -1.0011e+00,  1.9785e-01,  1.9413e-01,\n        -5.4273e-01,  6.2195e-01, -2.0900e-01, -1.9108e-02,  7.1911e-01,\n         1.1280e+00,  1.4576e-01,  3.0388e-01, -3.3991e-01, -1.2060e-01,\n        -5.0337e-02,  3.8178e-01, -1.3155e-01, -2.9325e-01, -3.9343e-01,\n        -3.0532e-01, -6.5886e-01, -4.7980e-01,  8.3746e-01, -1.9132e-01,\n         7.1522e-01,  4.0808e-02,  2.7025e-01, -3.1899e-01, -4.2074e-01,\n         3.0541e-01,  2.5962e-02,  6.9878e-01,  5.6418e-01,  1.5772e-02,\n        -4.5784e-01, -2.9554e-01,  8.9609e-01,  4.9120e-01, -5.2059e-01,\n         2.6763e-01,  2.9848e-01, -2.3999e-01, -8.5410e-01, -3.0280e-01,\n        -5.0677e-01,  4.3327e-01,  4.8044e-01,  2.9996e-01, -3.8550e-01,\n        -9.7982e-02,  1.0304e-01,  6.2428e-01, -2.4295e-01, -1.0584e-01,\n         1.7789e-01, -9.1307e-03, -1.3309e+00, -9.0027e-01,  8.5889e-01,\n         6.7263e-01, -1.9286e-01,  1.3732e+00, -2.2483e-01,  4.3873e-02,\n         1.2735e-02, -8.5651e-02,  4.6405e-01,  5.5733e-01, -3.9785e-01,\n         8.3131e-01, -3.6366e-01,  5.2756e-01, -5.7239e-01,  1.5378e-01,\n        -1.9711e-01, -2.7867e-02, -3.6705e-01, -6.5723e-01,  6.0061e-02,\n        -1.6265e-01,  2.4655e-02, -4.7020e-01,  3.0926e-02, -5.0243e-01,\n         8.6630e-01,  1.4816e+00, -6.9478e-01,  5.4247e-02, -7.6793e-01,\n         1.1224e-01, -2.1867e-02,  4.4579e-03,  1.0124e-01,  9.0423e-01,\n         3.0622e-01,  2.2643e-01,  2.2957e-01,  2.1617e-01, -4.1380e-01,\n        -1.3937e-01,  4.3693e-01, -8.0772e-01, -1.0891e+00,  2.5264e-01,\n         7.7390e-01, -9.2895e-01, -4.0141e-02,  9.5688e-01, -1.5233e+00,\n        -5.9582e-02,  8.2700e-01,  4.7387e-01, -3.7649e-02,  7.0751e-01,\n        -4.8861e-01, -4.0883e-01,  9.3142e-01, -5.0427e-01,  1.6424e-01,\n        -7.8574e-01,  6.8044e-01,  2.9842e-01, -4.0191e-01,  1.3222e-01,\n         7.1980e-01, -1.3141e-01,  1.0044e+00, -6.1877e-01,  6.1186e-02,\n        -3.0961e-01,  6.6428e-01, -7.9968e-01, -5.6750e-01,  1.2179e+00,\n        -7.7434e-01, -6.7217e-01, -5.4831e-01,  2.8635e-01, -8.5888e-01,\n         1.9030e-02, -1.7632e-02, -3.0232e-01,  2.8719e-01, -5.9644e-01,\n        -3.5699e-01, -5.3775e-01, -8.1116e-01, -5.5816e-01, -5.8424e-02,\n        -3.7239e-01, -1.0006e+00,  2.5986e-01,  6.2869e-01,  9.3768e-02,\n        -4.1999e-01,  1.1462e-01, -3.4222e-01,  7.0757e-01,  1.1912e+00,\n         1.4153e+00, -1.0829e+00,  1.7734e-01,  2.8735e-01, -4.3934e-01,\n        -8.2054e-01,  6.3801e-02, -5.2086e-01,  1.1607e+00, -5.4657e-01,\n        -2.8294e-02,  2.9137e-01,  3.2615e-01, -6.2965e-02, -1.1774e+00,\n         6.4628e-01, -5.5625e-01, -6.7123e-01, -3.0132e-01,  1.3541e-01,\n        -1.1217e-01, -6.6530e-01, -5.2717e-01, -5.4628e-01, -1.0702e+00,\n         1.1240e+00, -4.4346e-01,  6.1300e-01, -8.4763e-01,  8.7128e-01,\n         4.2580e-03,  4.8043e-01,  4.6288e-01, -5.5094e-02,  1.0245e-01,\n         1.6180e-01,  5.9468e-01,  3.9627e-01,  2.6382e-01,  5.1984e-04,\n        -1.1386e+00,  1.6584e-01,  6.8969e-02, -1.8366e-01,  1.9463e-01,\n        -6.2350e-01, -3.8378e-01, -8.3848e-01, -7.8976e-01, -1.1468e-01,\n         3.1708e-01,  1.6866e-01,  3.4936e-02,  1.4159e+00,  7.7591e-01,\n         2.4021e-01, -2.9985e-01,  1.2236e-01, -1.2237e-01,  9.2296e-01,\n         8.8469e-01,  4.5214e-01,  1.6373e-01,  1.0482e+00,  6.6342e-02,\n         4.0140e-01, -1.8976e-01,  3.5502e-01,  5.7205e-01,  7.1892e-02,\n        -3.7183e-03, -1.3134e-01,  8.7392e-01,  2.5273e-01, -7.0639e-01,\n        -1.2783e-01,  7.8148e-01,  9.2739e-01, -3.7166e-01, -9.1842e-01,\n        -1.0178e-01, -3.5030e-01,  8.6983e-01, -1.1560e+00,  1.1299e+00,\n        -2.6035e-01, -3.8845e-01, -4.5793e-01,  3.7977e-01,  1.6731e-01,\n        -5.2120e-01, -1.0793e+00, -4.5380e-01,  3.5265e-01, -3.9039e-01,\n        -1.2015e-01, -6.2439e-01,  3.3574e-01, -6.1461e-01,  3.4958e-01,\n         1.0341e+00, -4.4489e-02, -1.0011e+00, -3.9404e-01,  3.1194e-01,\n         9.4042e-01, -2.3928e-01, -1.2745e-02, -2.9051e-01, -4.0755e-01,\n        -8.3481e-01,  4.9792e-01, -3.2931e-01,  4.8752e-01,  8.4150e-01,\n         1.5479e+00,  1.2019e+00,  3.0994e-01, -1.7244e-01,  3.5600e-01,\n         4.6378e-01, -4.9641e-01,  4.9425e-01,  8.8362e-01,  5.3041e-01,\n         6.0492e-01,  6.2658e-01,  9.9128e-01, -9.9832e-03, -3.0573e-01,\n         1.6278e-03,  5.9301e-02,  5.6559e-01, -1.0437e-01,  2.9114e-01,\n         1.8318e-01,  2.0716e-02, -1.0573e-01, -2.6644e-01,  5.3573e-01,\n         2.5405e-01, -5.4540e-01,  9.1124e-01,  8.4726e-01,  5.2388e-01,\n         8.7099e-01,  9.4255e-01,  8.2686e-01, -7.4970e-02, -2.6913e-01,\n        -6.4876e-02, -2.6956e-01, -3.8780e-01, -5.0922e-01, -8.4957e-01,\n        -5.3192e-01, -7.8492e-01, -3.1380e-01,  2.8425e-01,  1.3782e-01,\n        -6.9144e-01, -4.4824e-01, -3.3231e-01, -4.4332e-02, -1.6109e-01,\n        -1.2751e+00, -5.3517e-01, -8.9070e-01,  2.2743e-01,  2.9663e-01,\n        -5.3890e-01, -4.6250e-01, -3.5436e-01,  2.5076e-01,  2.3247e-02,\n         2.8367e-01,  1.4163e-01, -6.0507e-01, -8.6597e-01,  7.4012e-01,\n         7.8962e-02,  6.0313e-01, -1.0688e+00, -2.3046e-01,  2.0131e-02,\n         1.2354e+00,  6.6587e-01, -8.8086e-01, -1.0734e+00,  8.0274e-01,\n         1.5737e-01,  4.0882e-01, -2.5072e-03,  1.8501e-01, -1.1503e-01,\n         2.5781e-01, -4.6916e-01,  2.0002e-01,  4.1378e-02, -2.9505e-01,\n        -3.0581e-01,  1.4649e+00,  8.5921e-01,  6.1749e-01, -2.0378e-01,\n        -5.9264e-02,  4.0399e-01, -5.8247e-02, -1.1654e-01,  6.8757e-01,\n         6.3216e-01,  6.3668e-02, -3.9217e-01, -5.5532e-01, -1.3047e-01,\n         1.0035e+00, -2.4000e-01, -3.1236e-01, -1.0721e+00,  8.5999e-01,\n        -1.6732e-01,  4.2085e-01,  1.6750e-02,  1.9552e-01,  2.0083e-01,\n         3.3335e-01,  5.2615e-01,  1.3316e-01,  3.7254e-01,  4.3134e-02,\n         1.0923e+00, -3.0019e-01, -1.1138e+00,  4.1822e-01, -4.5256e-01,\n         1.8726e-01, -6.9930e-01,  3.5127e-01, -1.7323e-01,  2.7880e-01,\n         8.7601e-02, -5.4727e-01, -5.8646e-01,  4.5214e-01, -9.5171e-01,\n        -1.8967e+00,  3.5379e-01, -1.3409e+00,  7.6501e-01,  3.4963e-01,\n         4.2164e-01, -2.1901e-01,  1.2907e+00, -8.1967e-01,  3.2864e-01,\n         1.2158e+00,  3.8136e-01,  1.3093e+00, -1.5533e-01, -1.1231e+00,\n        -4.5976e-01, -1.1575e-01,  1.1468e+00,  3.3271e-01,  3.8858e-01,\n         1.4051e-01, -7.3602e-01,  8.9081e-01,  1.0306e-01,  4.1540e-01,\n        -6.0454e-01, -3.5458e-01, -4.1095e-01, -1.1105e+00,  1.2175e-01,\n         9.1470e-01,  7.6459e-01,  4.1751e-01, -1.2235e+00,  8.5953e-02,\n        -4.9345e-01,  3.3967e-01,  1.0174e+00,  4.2226e-01, -4.7855e-01,\n         4.9780e-01,  4.1484e-01, -4.0739e-01,  1.3534e-01, -4.9061e-02,\n         5.4594e-01, -2.8843e-01, -3.9948e-01,  3.5894e-01,  7.4281e-01,\n         3.2530e-01,  7.2176e-02, -1.6886e-01,  6.5054e-02, -2.4910e-01,\n         2.8944e-01, -3.7473e-01, -3.6072e-01, -8.1856e-02, -3.3686e-01,\n         3.0079e-01, -6.9607e-01, -1.1114e-02,  3.2690e-01,  2.8685e-01,\n        -2.6537e-01, -1.5346e-01,  8.3861e-01,  1.7236e-01, -3.4668e-02,\n        -7.2764e-01,  7.5580e-01, -4.1560e-01, -3.5155e-01, -4.2521e-01,\n         2.6169e-01,  1.1017e+00, -4.5022e-01, -7.9423e-01,  5.2791e-02,\n         1.1211e-02,  7.0663e-01, -7.6058e-01, -3.3632e-01,  6.2192e-01,\n         3.9273e-01,  5.3159e-01, -9.1834e-01, -2.0929e-03,  4.1401e-01,\n        -6.4326e-01, -1.0512e+00, -5.0625e-01,  4.4696e-01,  9.5999e-02,\n        -1.6435e+00, -3.9428e-01,  2.2786e-01, -4.4352e-01, -4.7796e-01,\n        -8.9078e-01, -1.2434e-02, -1.7850e-01, -3.9252e-01,  2.4213e-01,\n        -2.9037e-01, -6.2621e-01, -1.0240e-01, -2.0067e-01, -1.6675e-01,\n        -5.0272e-01, -2.4376e-01, -4.6624e-01, -2.7546e-01, -2.1753e-02,\n         2.1764e-01, -6.0180e-01, -6.0241e-01, -1.7905e-01, -2.8081e-01,\n         4.7748e-01,  1.0485e+00,  1.5767e-01,  1.2524e+00,  5.2022e-01,\n         4.6058e-01, -1.0614e-01,  8.9280e-01, -3.4498e-01,  1.5320e-01,\n         4.1233e-01, -6.1691e-01,  5.5570e-01,  3.2078e-01, -5.6668e+00,\n         7.4986e-01, -4.4440e-01,  5.2912e-01,  9.8795e-02,  2.0004e-01,\n         4.7810e-01, -3.6460e-01,  5.4335e-01,  7.7406e-01, -2.6943e-01,\n        -7.4687e-01, -6.8795e-01,  4.5029e-02,  6.1682e-01,  4.6696e-01,\n         4.4347e-01, -9.3197e-01,  4.3553e-01,  4.8560e-01, -4.9842e-01,\n         2.1000e-01, -1.2379e+00,  7.1226e-01,  6.2223e-02,  2.7775e-01,\n         2.7940e-01, -7.3657e-01,  7.3758e-01, -7.8994e-02, -3.2220e-01,\n        -1.0982e-01,  8.7814e-01,  4.8609e-01, -6.5768e-02,  5.0536e-01,\n        -3.0308e-01, -7.0505e-02,  5.5840e-01, -9.1126e-01, -4.1461e-01,\n        -2.2437e-01,  6.7728e-01,  1.5337e-01], device='cuda:0')"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}